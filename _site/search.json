[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HarvardX - Data Science: Capstone",
    "section": "",
    "text": "This website includes the final two projects of the capstone course for the HarvardX Professional Certificate Program for Data Science. Below is a summary of each project."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Sara Brady, and I am a policy researcher for a nonprofit, First Five Nebraska. I used to be an associate professor of psychology for many years after earning a Ph.D. in Experimental Psychology from Texas Christian University. I’ve taught countless students about research methods, statistics, and social and personality psychology. I’ve also advised numerous academic and nonprofit researchers on research design and analysis as a consultant. Currently, I use quantitative and qualitative methodologies to understand policies and legislation affecting young children. My work informs policymakers on issues affecting early care and education, as well as maternal and infant health."
  },
  {
    "objectID": "index.html#movielens-project",
    "href": "index.html#movielens-project",
    "title": "HarvardX - Data Science: Capstone",
    "section": "1. MovieLens Project",
    "text": "1. MovieLens Project\nThe goal of this project is to use the frequently-cited MovieLens dataset and employ machine learning principles to achieve a particular level of accuracy in predicting movie ratings."
  },
  {
    "objectID": "index.html#choose-your-own-data",
    "href": "index.html#choose-your-own-data",
    "title": "HarvardX - Data Science: Capstone",
    "section": "2. Choose Your Own Data",
    "text": "2. Choose Your Own Data\nThe goal of this project was to choose your own dataset to use a machine learning task."
  },
  {
    "objectID": "index.html#nebraska-state-legislature",
    "href": "index.html#nebraska-state-legislature",
    "title": "HarvardX - Data Science: Capstone",
    "section": "2. Nebraska State Legislature",
    "text": "2. Nebraska State Legislature\nThe goal of this project is for each student to choose their own dataset and employ machine learning techniques to predict a given outcome. For this project, I identify relevant early childhood legislation introduced in the Nebraska Legislature and use machine learning tasks to predict whether the legislation passes."
  },
  {
    "objectID": "ne-legis.html",
    "href": "ne-legis.html",
    "title": "Nebraska Legislature",
    "section": "",
    "text": "Project Overview\nForthcoming"
  },
  {
    "objectID": "movielens.html",
    "href": "movielens.html",
    "title": "Capstone MovieLens Project",
    "section": "",
    "text": "The MovieLens dataset (GroupLens Research, 2009) contains user movie ratings from MovieLens, a movie recommendation service developed by a research lab at the University of Minnesota (GroupLens Research, 2025). For this project, I created a movie recommendation system using the 10-million version of this dataset.\nThe goal was to use machine learning to generate predicted movie ratings and root mean squared error (RMSE) score. Below are the steps that were used to:\n\nCreate the main training and final hold-out test datasets.\nTrain a machine learning algorithm using the main training set.\nSelect a final model based upon the main training set.\nPredict movie ratings using the final hold-out test set.\nEvaluate the accuracy of the generated predictions using the RMSE metric.\n\nThis project simulates on a smaller scale the 2006 Netflix competition, whereby Netflix released a dataset containing 100 million de-identified movie ratings and offered $1 million to whoever could develop an algorithm that was better than the accuracy of its own recommendation system (Bennett & Lanning, 2007)."
  },
  {
    "objectID": "movielens.html#creating-main-training-and-final-hold-out-datasets",
    "href": "movielens.html#creating-main-training-and-final-hold-out-datasets",
    "title": "Capstone MovieLens Project",
    "section": "Creating Main Training and Final Hold-out Datasets",
    "text": "Creating Main Training and Final Hold-out Datasets\nThe following code was developed by the HarvardX Team for the course, PH125.9x: Data Science: Capstone. The purpose of creating the edx dataset is to ensure reproducibility to train, develop, and select the final algorithm used for evaluating the RMSE. We will use this dataset for separating into training and test sets, as well as cross-validation in designing and testing the final algorithm. The final_holdout_test dataset will be used to test the final algorithm.\n\n##########################################################\n# Create edx and final_holdout_test sets \n##########################################################\n# MovieLens 10M dataset:\n# https://grouplens.org/datasets/movielens/10m/\n# http://files.grouplens.org/datasets/movielens/ml-10m.zip\n\noptions(timeout = 120)\n\ndl &lt;- \"ml-10M100K.zip\"\nif(!file.exists(dl))\n  download.file(\"https://files.grouplens.org/datasets/movielens/ml-10m.zip\", dl)\n\nratings_file &lt;- \"ml-10M100K/ratings.dat\"\nif(!file.exists(ratings_file))\n  unzip(dl, ratings_file)\n\nmovies_file &lt;- \"ml-10M100K/movies.dat\"\nif(!file.exists(movies_file))\n  unzip(dl, movies_file)\n\nratings &lt;- as.data.frame(str_split(read_lines(ratings_file), fixed(\"::\"), simplify = TRUE),\n                         stringsAsFactors = FALSE)\ncolnames(ratings) &lt;- c(\"userId\", \"movieId\", \"rating\", \"timestamp\")\nratings &lt;- ratings %&gt;%\n  mutate(userId = as.integer(userId),\n         movieId = as.integer(movieId),\n         rating = as.numeric(rating),\n         timestamp = as.integer(timestamp))\n\nmovies &lt;- as.data.frame(str_split(read_lines(movies_file), fixed(\"::\"), simplify = TRUE),\n                        stringsAsFactors = FALSE)\ncolnames(movies) &lt;- c(\"movieId\", \"title\", \"genres\")\nmovies &lt;- movies %&gt;%\n  mutate(movieId = as.integer(movieId))\n\nmovielens &lt;- left_join(ratings, movies, by = \"movieId\")\n\n# Final hold-out test set will be 10% of MovieLens data\nset.seed(1, sample.kind=\"Rounding\") # if using R 3.6 or later\n# set.seed(1) # if using R 3.5 or earlier\ntest_index &lt;- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)\nedx &lt;- movielens[-test_index,]\ntemp &lt;- movielens[test_index,]\n\n# Make sure userId and movieId in final hold-out test set are also in edx set\nfinal_holdout_test &lt;- temp %&gt;% \n  semi_join(edx, by = \"movieId\") %&gt;%\n  semi_join(edx, by = \"userId\")\n\n# Add rows removed from final hold-out test set back into edx set\nremoved &lt;- anti_join(temp, final_holdout_test)\nedx &lt;- rbind(edx, removed)\n\nrm(dl, ratings, movies, test_index, temp, movielens, removed)\n\nsaveRDS(edx, file = \"../9_Capstone/data/edx.rds\")\nsaveRDS(final_holdout_test, file = \"../9_Capstone/data/final_holdout_test.rds\")"
  },
  {
    "objectID": "movielens.html#data-exploration",
    "href": "movielens.html#data-exploration",
    "title": "Capstone MovieLens Project",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nBasic Structure\nFrom the training dataset we created above, we see that there are more than 9 million rows of data with six variables: userId, movieId, rating, timestamp, title, and genres. This data also appears to be organized into a long format, meaning that rows of data include repeated values of the first column (userId).\nOur ultimate goal is to use this dataset to predict users’ movie ratings in the final hold-out test set. We will need to use data on users, movies, and ratings to predict the outcome variable, but other variables such as when the rating was given (timestamp) or genres may also be useful. The variable titles will be useful when exploring the data descriptively, but is likely not going to be useful since there will undoubtedly be duplicate movie titles. To index movies, the movieId variable will be the most useful.\n\nstr(edx)\n\n\n## 'data.frame':    9000055 obs. of  6 variables:\n##  $ userId   : int  1 1 1 1 1 1 1 1 1 1 ...\n##  $ movieId  : int  122 185 292 316 329 355 356 362 364 370 ...\n##  $ rating   : num  5 5 5 5 5 5 5 5 5 5 ...\n##  $ timestamp: int  838985046 838983525 838983421 838983392 838983392 838984474 838983653 838984885 838983707 838984596 ...\n##  $ title    : chr  \"Boomerang (1992)\" \"Net, The (1995)\" \"Outbreak (1995)\" \"Stargate (1994)\" ...\n##  $ genres   : chr  \"Comedy|Romance\" \"Action|Crime|Thriller\" \"Action|Drama|Sci-Fi|Thriller\" \"Action|Adventure|Sci-Fi\" ...\n\n\n\nDistinct Values\nAs mentioned earlier, a long data format means that ID variables are repeated. Therefore, to determine how many users, movies, and genres are included in this dataset, we use the dplyr package to count distinct values in each variable. From this, we see that over 69,000 users rated more than 10,000 movies from 797 unique genres.\n\nedx_ns &lt;- edx |&gt; \n  reframe(n_users = n_distinct(userId),\n          n_movies = n_distinct(movieId),\n          n_genres = n_distinct(genres))\n\n\nedx_ns\n##   n_users n_movies n_genres\n## 1   69878    10677      797\n\n\n\nRatings\nTo explore our outcome variable, we see that ratings range from 0.5 to 5.0.\n\nrange(edx$rating)\n\n\n## [1] 0.5 5.0\n\nWe will store the average rating so that we can visualize ratings in a plot. A histogram shows the negative skew of ratings. Although the average rating is about 3.5, the most common rating was 4 with over 2.5 million ratings in total.\n\nmu &lt;- mean(edx$rating)\n\nedx |&gt; \n  group_by(rating) |&gt; \n  count() |&gt; \n  ggplot() +\n  geom_col(aes(rating, n)) +\n  geom_vline(xintercept = mu,\n             linetype = 2) +\n  annotate(geom = \"text\", x = 2.8, y = 3000000,\n           label = paste0(\"Mean = \", round(mu, 3)),\n           color = \"red\")\n\n\n\n\nRatings per User\nTo determine the number of ratings per user, we count rows by userId. Descriptive statistics suggest that the number of user ratings varies considerably with the standard deviation being larger than the mean.\n\nratings_users &lt;- edx |&gt; \n  count(userId) |&gt; \n  reframe(N = n(),\n          Mean = mean(n),\n          Median = median(n),\n          SD = sd(n),\n          Min = min(n),\n          Max = max(n),\n          IQR.25 = quantile(n, 0.25),\n          IQR.75 = quantile(n, 0.75)) |&gt; \n  pivot_longer(everything(), names_to = \"statistic\")\n\nratings_users\n\n\n## # A tibble: 8 × 2\n##   statistic  value\n##   &lt;chr&gt;      &lt;dbl&gt;\n## 1 N         69878 \n## 2 Mean        129.\n## 3 Median       62 \n## 4 SD          195.\n## 5 Min          10 \n## 6 Max        6616 \n## 7 IQR.25       32 \n## 8 IQR.75      141\n\nSince the number of ratings per user is so positively skewed, transforming the x axis into log values allows us to see the distribution more clearly.\n\navg_ratings_per_user &lt;- ratings_users$value[which(ratings_users$statistic==\"Mean\")]\n\ncount(edx, userId) |&gt; \n  ggplot() +\n  geom_histogram(aes(n), \n                 bins = 60,\n                 color = \"lightgray\") +\n  scale_x_log10() +\n  geom_vline(xintercept = avg_ratings_per_user,\n             linetype = 2) +\n  annotate(geom = \"text\", x = 350, y = 3500,\n           label = paste0(\"Mean = \", round(avg_ratings_per_user, 3)),\n           color = \"red\")\n\n\n\n\nRatings per Movie\nAs expected, the number of ratings per movie was heavily positively skewed with some movies being much more popular than others.\n\nratings_movies &lt;- edx |&gt; \n  count(movieId)\n\nratings_movies_summary &lt;- ratings_movies |&gt; \n  reframe(N = n(),\n          Mean = mean(n),\n          Median = median(n),\n          SD = sd(n),\n          Min = min(n),\n          Max = max(n),\n          IQR.25 = quantile(n, 0.25),\n          IQR.75 = quantile(n, 0.75)) |&gt; \n  pivot_longer(everything(), names_to = \"statistic\")\n\n\nratings_movies_summary\n## # A tibble: 8 × 2\n##   statistic  value\n##   &lt;chr&gt;      &lt;dbl&gt;\n## 1 N         10677 \n## 2 Mean        843.\n## 3 Median      122 \n## 4 SD         2238.\n## 5 Min           1 \n## 6 Max       31362 \n## 7 IQR.25       30 \n## 8 IQR.75      565\n\nOver 100 movies only had a single rating, whereas the most frequently-rated movie, Pulp Fiction, had 31,362 ratings.\n\nfilter(ratings_movies, n == 1) |&gt; \n  nrow()\n\n\n## [1] 126\n\n\nmost_ratings &lt;- ratings_movies |&gt; \n  left_join(select(edx, movieId, title) |&gt; distinct(), \n            by = \"movieId\") |&gt; \n  filter(n == max(n))\n\n\nmost_ratings\n##   movieId     n               title\n## 1     296 31362 Pulp Fiction (1994)\n\nThe histogram (log-scale transformed) reveals that the average number of ratings per movie is somewhere between 50 and 500, despite the fact that the mean is more than 800. Based upon this data and the data above, we can infer that there are going to be many instances where users did not rate a given movie. The challenge, therefore, is to estimate what their ratings would be on all 10,000+ movies.\n\navg_ratings_per_movie &lt;- ratings_movies_summary |&gt; \n  filter(statistic == \"Mean\") |&gt; \n  pull(value)\n\ncount(edx, movieId) |&gt; \n  ggplot() +\n  geom_histogram(aes(n), \n                 bins = 60,\n                 color = \"lightgray\") +\n  scale_x_log10() +\n  geom_vline(xintercept = avg_ratings_per_movie,\n             linetype = 2) +\n  annotate(geom = \"text\", x = 4275, y = 390,\n           label = paste0(\"Mean = \", round(avg_ratings_per_movie, 3)),\n           color = \"red\")"
  },
  {
    "objectID": "movielens.html#creating-test-and-training-sets-for-model-testing",
    "href": "movielens.html#creating-test-and-training-sets-for-model-testing",
    "title": "Capstone MovieLens Project",
    "section": "Creating Test and Training Sets for Model Testing",
    "text": "Creating Test and Training Sets for Model Testing\nEven though we created the training (edx) and testing (final_holdout_test) datasets, we will only use the final_holdout_test dataset after choosing the final machine learning algorithm. Therefore, we will use the edx dataset to create training and testing partitions to be used during model training. Using the caret package, we generate the indexes for randomly dividing the data into two parts. Then, we create the training and test sets using the indexes generated. The methodology here uses the same logic that the HarvardX Team used above for creating the edx and final_holdout_test sets.\nFirst, we use the caret’s createDataPartition() function to generate indices that are used to subset 90% of the edx dataset into the main training set. Then, we create a temp dataset that will serve as the basis of the indices generated by the data partition, representing 10% of edx data. To ensure that all users and movies appear in the test set, we use dplyr’s semi_join() function to return all rows from temp with a match in train_set. Next, we use anti_join() to return all rows from temp that don’t appear in test_set. Finally, we use rbind()to combine the removed rows with the matched rows to produce our final test_set.\n\ny = edx$rating\nset.seed(7)\ntest_index &lt;- caret::createDataPartition(y, times = 1, p = 0.1, list = FALSE)\ntrain_set &lt;- edx[-test_index,]\ntemp &lt;- edx[test_index,]\n\ntest_set &lt;- temp |&gt; \n  semi_join(train_set, by = \"movieId\") |&gt; \n  semi_join(train_set, by = \"userId\")\n\nremoved &lt;- anti_join(temp, test_set)\ntrain_set &lt;- rbind(train_set, removed)\n\nsaveRDS(train_set, \"../../9_Capstone/data/train-set.rds\")\nsaveRDS(test_set,  \"../../9_Capstone/data/test-set.rds\")"
  },
  {
    "objectID": "movielens.html#rmse",
    "href": "movielens.html#rmse",
    "title": "Capstone MovieLens Project",
    "section": "RMSE",
    "text": "RMSE\nTo test model accuracy, we will use a commonly used metric for evaluating the extent to which predicted ratings matched actual ratings. When Netflix opened a competition for the best algorithm to predict user ratings (Lohr, 2009), they used the root mean square error (RMSE) to compare the best algorithm against their current algorithm used at the time (Netflix & Crawford, 2017). The winning algorithm achieved an RMSE of of 0.8712 (Bell et al., 2007).\nRMSE is often used as an evaluation metric of regression model accuracy. It answers the question: How accurate are a model’s predicted values? Because of how it is calculated, RMSE directly measures prediction error in the same units as the outcome variable. The $1 million prize-winning algorithm, therefore, estimated actual movie ratings within 0.8712 stars on the 5-star rating scale.\nThe mathematical formula for calculating RMSE is:\n\\[\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\]\nhere:\n\n\\(y_i\\) is the actual rating for observation \\(i\\),\n\\(\\hat{y}_i\\) is the predicted rating for observation \\(i\\),\n\\(n\\) is the total number of observations\n\nIn effect, RMSE calculates the residuals, squares them, finds the mean of the squared residuals, and takes the square root.\nThen, a simplified formula of RMSE is:\n\\[\n\\mathrm{RMSE} = \\sqrt{\\text{mean}\\Big((y_i - \\hat{y}_i)^2\\Big)}\n\\]\nhere:\n\n\\(y_i\\) is the vector of actual ratings\n\\(\\hat{y}_i\\) is the vector of predicted values\n\nWe can write a function in R to calculate RMSE. The predicted ratings will be generated from whatever model we choose based upon the training datset, and the actual ratings will come from the testing dataset.\n\nRMSE &lt;- function(actual_ratings, predicted_ratings){\n     sqrt(mean((actual_ratings - predicted_ratings)^2))\n}"
  },
  {
    "objectID": "movielens.html#average-model",
    "href": "movielens.html#average-model",
    "title": "Capstone MovieLens Project",
    "section": "Average Model",
    "text": "Average Model\nTo benchmark our models, we estimate the naive RMSE using the average rating in the training dataset to predict the test set’s ratings. This code was adapted from Introduction to Data Science (Irizarry, 2019).\nOur average model would look like this: \\(Y_{u,i} = \\mu + \\varepsilon_{u,i}\\) with \\(\\varepsilon_{u,i}\\) independent errors sampled from the same distribution centered at 0 and \\(\\mu\\), the actual “true” rating for all movies.\nSince we don’t know the “true” rating for all movies, we predict all unknown ratings with \\(\\hat{\\mu}\\) using the average of all ratings from the training set. Given this model, our benchmark RMSE is 1.0606.\n\nmu_hat &lt;- mean(train_set$rating)\nnaive_rmse &lt;- RMSE(test_set$rating, mu_hat)\n\n\nnaive_rmse\n## [1] 1.060636\n\nBy plotting the residuals, we see a plot that looks identical to the histogram of the ratings. This is because we have effectively centered all ratings around the mean.\n\nnaive_df &lt;- select(test_set, rating) |&gt; \n  mutate(residuals = rating - mu_hat)\n\nselect(naive_df, residuals) |&gt; \n  ggplot(aes(x = residuals)) +\n  geom_histogram(binwidth = 0.5,\n                 color = \"lightgray\") +\n  geom_vline(xintercept = 0,\n             linetype = 2)"
  },
  {
    "objectID": "movielens.html#movie-effect-model",
    "href": "movielens.html#movie-effect-model",
    "title": "Capstone MovieLens Project",
    "section": "Movie Effect Model",
    "text": "Movie Effect Model\nNow that we have our naive RMSE to benchmark all other models, we start with modeling movie effects (also called bias) using linear regression. Here we add to our previous model a bias term \\(b_i\\) to represent the average rating for movie \\(i\\):\n\\[\nY_{u,i} = \\mu + b_i + \\varepsilon_{u,i}\n\\] here:\n\n\\(Y_{u,i}\\) is the rating given by user \\(u\\) of movie \\(i\\)\n\\(\\mu\\) is the “true” rating for all movies\n\\(b_i\\) is the “true” rating for movie \\(i\\)\n\\(\\varepsilon_{u,i}\\) are independent errors sampled from the same distribution centered at 0\n\nRemember that in our RMSE equation we need to calculate \\(\\hat{y}_i\\), the predicted rating for observation \\(i\\). Our goal, then, is to find the values that minimize the distance between our fitted model and the data (i.e., the least squares estimates). Here our model suggests that we can obtain our least squares estimates by taking the average for all ratings and add the average rating of each movie \\(i\\). The most common way to do this in R is by using the lm function as shown below. However, do not run this code. We have over 10,000 movies and each movie would be a predictor in the model, meaning that R would attempt to calculate over 10,000 coefficient estimates, standard errors, t statistics, and p values.\n\n# DO NOT RUN\nfit &lt;- lm(rating ~ movieId, data = train_set)\n\nBecause the least squares estimate \\(\\hat{b}_i\\) is simply the average of each user’s rating of movie \\(i\\) minus the average of all ratings, we can calculate \\(\\hat{b}_i\\) as follows.\n\nmovie_avgs &lt;- train_set |&gt;  \n     group_by(movieId) |&gt;  \n     summarize(avg_rating = mean(rating),\n               b_i_hat = mean(rating - mu_hat))\n\nWe then, calculate the predicted values as our model indicates by adding \\(\\hat{\\mu}\\) (average of all ratings) to \\(\\hat{b}_i\\) (average rating of each movie).\n\npredicted_ratings &lt;- mu_hat + test_set |&gt; \n  left_join(movie_avgs, by=\"movieId\") |&gt; \n     pull(b_i_hat)\n\nWith our predicted ratings from our movie effects model, we see that our RMSE improves.\n\nfit1_rmse &lt;- RMSE(predicted_ratings, test_set$rating)\n\n\nfit1_rmse\n## [1] 0.9441454\n\nAlthough our accuracy metric improved, our predicted ratings were only accurate within 0.944 points. To explore why relying on movie effects alone may be insufficient, we can calculate the top 1% of movies and store these movie’s ID numbers. In this case, the top 1% of movies refers to movies that had average ratings in the top 1 percentile of all movies. We, then, create a data frame that includes the counts of the number of user ratings.\n\ntop_movies &lt;- movie_avgs |&gt; \n  filter(b_i_hat &gt; quantile(b_i_hat, .99)) |&gt; \n  pull(movieId)\n\ntop1perc &lt;- train_set |&gt; \n  left_join(movie_avgs, by = \"movieId\") |&gt; \n  filter(movieId %in% top_movies) |&gt; \n  group_by(movieId, title) |&gt; \n  reframe(n = n(), avg_rating) |&gt; \n  distinct()\n\nThere were 107 movies that were ranked in the top 1% with average ratings at or above 4.1947.\n\nnrow(top1perc)\n## [1] 107\nrange(top1perc$avg_rating)\n## [1] 4.194669 5.000000\n\nIf we look at a random sample of this data frame we see that the number of ratings vary considerably by movie.\n\ntop1perc |&gt; \n  slice_sample(n = 10)\n## # A tibble: 10 × 4\n##    movieId title                                                    n avg_rating\n##      &lt;int&gt; &lt;chr&gt;                                                &lt;int&gt;      &lt;dbl&gt;\n##  1    3022 General, The (1927)                                    890       4.28\n##  2   53883 Power of Nightmares: The Rise of the Politics of Fe…     4       4.5 \n##  3     670 World of Apu, The (Apur Sansar) (1959)                 348       4.22\n##  4   63179 Tokyo! (2008)                                            1       4.5 \n##  5     904 Rear Window (1954)                                    7172       4.32\n##  6    2935 Lady Eve, The (1941)                                   608       4.20\n##  7    1212 Third Man, The (1949)                                 2668       4.31\n##  8      50 Usual Suspects, The (1995)                           19384       4.36\n##  9     913 Maltese Falcon, The (1941)                            5354       4.25\n## 10   33264 Satan's Tango (Sátántangó) (1994)                        2       5\n\nIn fact, we see that among the top rated 1% of movies, almost half of them were only rated once. This would be analogous to a user sorting movies by rating and then seeing that a large proportion of them had only a single rating.\n\ntop1perc |&gt; \n  ggplot(aes(n)) +\n  geom_histogram(binwidth = 500,\n                 color = \"lightgray\") +\n  labs(title = \"Frequency of Ratings for Top 1% of Movies\",\n       x = \"Number of Ratings per Movie\")\n\n\nWe can also perform a check on the residuals. If the relationship between the ratings and the movie ratings is linear, we should see that there is no relationship between residuals and predicted values when plotting them. With a continuous outcome variable, we would see the residuals “bounce” around the 0 line. Since our outcome variable is on a 5-point scale, we do not see this pattern. However, given the lack of curvature and that the best-fit line is 0, we see that the variances of the error terms are equal. There do appear to be some outliers, such that we predicted 5s but the actual ratings were much lower.\n\nfit1_resid &lt;- test_set |&gt; \n  left_join(movie_avgs, by='movieId') |&gt; \n  mutate(predicted_ratings = mu_hat + b_i_hat,\n         residuals = rating - predicted_ratings)\n\np_fit1_resid &lt;- fit1_resid |&gt; \n  ggplot(aes(x = predicted_ratings, y = residuals)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Residual Plot for Movie Effects Model\")\n\np_fit1_resid"
  },
  {
    "objectID": "movielens.html#movie-user-effects-model",
    "href": "movielens.html#movie-user-effects-model",
    "title": "Capstone MovieLens Project",
    "section": "Movie & User Effects Model",
    "text": "Movie & User Effects Model\nSince we know that users have a bias in how they rate movies, we will estimate the user bias by adding the term \\(b_u\\) to represent the average rating for user \\(u\\) to our model:\n\\[\nY_{u,i} = \\mu + b_i + b_u + \\varepsilon_{u,i}\n\\] here:\n\n\\(Y_{u,i}\\) is the rating given by user \\(u\\) of movie \\(i\\)\n\\(\\mu\\) is the “true” rating for all movies\n\\(b_i\\) is the “true” rating for movie \\(i\\)\n\\(b_u\\) is the “true” rating for user \\(u\\)\n\\(\\varepsilon_{u,i}\\) are independent errors sampled from the same distribution centered at 0\n\nSimilar to before, we can estimate the least squares estimate \\(\\hat{b}_u\\) by taking the mean of \\(y_{u,i} - \\hat{\\mu} - \\hat{b}_i\\) where \\(y_{u,i}\\) is the rating of movie \\(i\\) from user \\(u\\), \\(\\hat{\\mu}\\) is the average of all ratings, and \\(\\hat{b}_i\\) is the least squares estimate of the movie effect we calculated earlier.\n\nuser_avgs &lt;- train_set %&gt;% \n     left_join(movie_avgs, by='movieId') %&gt;%\n     group_by(userId) %&gt;%\n     summarize(b_u_hat = mean(rating - mu_hat - b_i_hat))\n\nTo ensure our predicted values are restricted to values between 0.5 and 5, we include a clamp function.\n\nclamp &lt;- function(x, min = 0.5, max = 5) pmax(pmin(x, max), min)\n\npredicted_ratings &lt;- test_set |&gt; \n     left_join(movie_avgs, by=\"movieId\") |&gt; \n     left_join(user_avgs, by=\"userId\") |&gt; \n     mutate(pred = clamp(mu_hat + b_i_hat + b_u_hat)) %&gt;%\n     pull(pred)\n\nWe see that our RMSE has improved even more. We can now estimate within 0.8659 points users’ ratings.\n\nfit2_rmse &lt;- RMSE(predicted_ratings, test_set$rating)\n\n\nfit2_rmse\n## [1] 0.8658849\n\nAlthough our residuals look better (no outliers), this model still does not address the fact that less popular movies have least squares estimates as high or higher than popular movies. We need a model that could somehow ignore movie averages with a few user ratings and replace those movie averages with the average of all movies. This is the idea behind regularization.\n\nfit2_resid &lt;- test_set |&gt; \n  left_join(movie_avgs, by=\"movieId\") |&gt; \n  left_join(user_avgs, by=\"userId\") |&gt; \n  mutate(predicted_ratings = clamp(mu_hat + b_i_hat + b_u_hat),\n         residuals = rating - predicted_ratings)\n\nfit2_resid |&gt; \n  ggplot(aes(x = predicted_ratings, y = residuals)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Residual Plot for Movie & User Effects Model\")"
  },
  {
    "objectID": "movielens.html#regularization-of-movie-user-effect",
    "href": "movielens.html#regularization-of-movie-user-effect",
    "title": "Capstone MovieLens Project",
    "section": "Regularization of Movie & User Effect",
    "text": "Regularization of Movie & User Effect\nThe concept of regularization takes into account each predictor’s effect size when calculating their least squares estimates. In this case, we assume that movies or users with a large number of ratings are more reliable than movies or users with a small number of ratings. When calculating the mean of the least squares estimate, we add a penalty term \\(\\lambda\\) to the sample size in the denominator. Therefore, in the case of estimating the movie effect term \\(b_i\\), we previously calculated \\(mean(y_{u,i} - \\hat{\\mu})\\) or written out in summation notation:\n\\[\n\\frac{1}{n}\\sum_{i=1}^{n} (y_{u,i} - \\hat{\\mu})\n\\]\nHowever with a penalized least squares estimate of \\(b_i\\), we add \\(\\lambda\\) as a penalty term to the denominator:\n\\[\n\\frac{1}{n+\\lambda}\\sum_{i=1}^{n} (y_{u,i} - \\hat{\\mu})\n\\]\nIf we use, for example, \\(\\lambda = 5\\) This means that for movies like The Shawshank Redemption with over 25,000 ratings in the training set, adding 5 to the n() calculation will be trivial. But for movies with only 1 rating, dividing by n() + 5 will mean that the least squares estimate will be essentially 0 after subtracting \\(\\hat{b}_i\\) from the average \\(\\hat{\\mu}\\).\nTo ensure that we select the optimal value of \\(\\lambda\\), we will write a function that estimates each of the terms in our model based upon different levels of lambdas. We, then, calculate the RMSE for each of the models that differ only based on the lambdas value. Because we are calculating these effects 41 times (length of lambdas), it’s a good idea to document processing time using the Sys.time() function.\n\nlambdas &lt;- seq(0, 10, 0.25)\n\nt1 &lt;- Sys.time()\n\nrmses &lt;- sapply(lambdas, function(l){\n     mu &lt;- mean(train_set$rating)\n     \n     b_i &lt;- train_set %&gt;% \n          group_by(movieId) %&gt;%\n          summarize(b_i = sum(rating - mu)/(n()+l))\n     \n     b_u &lt;- train_set %&gt;% \n          left_join(b_i, by=\"movieId\") %&gt;%\n          group_by(userId) %&gt;%\n          summarize(b_u = sum(rating - b_i - mu)/(n()+l))\n     \n     predicted_ratings &lt;- test_set %&gt;% \n          left_join(b_i, by = \"movieId\") %&gt;%\n          left_join(b_u, by = \"userId\") %&gt;%\n          mutate(pred = clamp(mu + b_i + b_u)) %&gt;%\n          pull(pred)\n     \n     return(RMSE(predicted_ratings, test_set$rating))\n})\n\nSys.time() - t1\n\n\n## Time difference of 1.942676 mins\n\nTo visualize whether we selected an appropriate range for \\(\\lambda\\), we plot the RMSE values against the values in lambdas.\n\nggplot(data.frame(lambdas, rmses)) +\n  geom_point(aes(lambdas, rmses))\n\n\nDespite the extra computation, we see that the \\(\\lambda\\) with the lowest RMSE was not much better than the RMSE from the non-regularized movie & user effects model: fit2_rmse = 0.8659. This suggests that we need a model that can take into account the patterns of users and movies beyond their averages.\n\nlambda_m3 &lt;- lambdas[which.min(rmses)]\nfit3_rmse &lt;- min(rmses)\n\n\nlambda_m3\n## [1] 4.25\nfit3_rmse\n## [1] 0.8654354"
  },
  {
    "objectID": "movielens.html#dimension-reduction-and-svd",
    "href": "movielens.html#dimension-reduction-and-svd",
    "title": "Capstone MovieLens Project",
    "section": "Dimension Reduction and SVD",
    "text": "Dimension Reduction and SVD\n\nProblem with Unexplained Variance\nDespite estimating movie-to-movie differences through \\(b_i\\) and user-to-user differences through \\(b_u\\), we are not estimating variation related to groups of movies and users with similar rating patterns. If we look at the residuals for some popular films, we see that non-zero correlations exist. This is expected for movies of the same franchise (Plot A) or the same genre (Plot B). We also see this for overlapping genres (Plot C), as well as non-overlapping genres (Plot D). Because residuals are supposed to be uncorrelated with each other, this suggests that our current model is not capturing certain commonalities between movies and users. Moreover, the commonalities that exist are not readily apparent from knowing the genre, release date, or title.\n\n\n\nPCA via SVD as Method for Dimension Reduction\nBecause we have 10,677 predictors (movies) in our model, a popular method for reducing the number of predictors is through principal component analysis (PCA). PCA seeks to identify groups of predictors, whereby predictors within groups are relatively similar and the groups are relatively independent of one another. In this case, the groups of predictors are called principal components (PCs).\nTraditionally, PCs are extracted using eigenvalue decomposition in which the \\(p \\times p\\) covariance matrix (i.e., the matrix of correlations between predictors) is computed and, then, transformed into a matrix of \\(p \\times p\\) eigenvalues (i.e., the variance explained by each PC) using matrix factorization via matrix algebra. See Penn State (2025) for an explanation of eigenvalue calculations.\nBecause calculating the covariance matrix is computationally intensive on large datasets, an alternative approach to extracting PCs is singular value decomposition (SVD). With SVD, we can avoid calculating the covariance matrix altogether and instead transform the dataset matrix, \\(\\textbf{X}\\), of size \\(n \\times p\\) (number of \\(n\\) users by number of \\(p\\) movies) into three special matrices. Two matrices are orthogonal (rotated): \\(\\textbf{U}\\) and \\(\\textbf{V}^\\top\\). A third matrix, \\(\\Sigma\\) is diagonal matrix.\nThe mathematical formula for SVD then is:\n\\[\n\\textbf{X} = \\textbf{U}\\Sigma\\textbf{V}^\\top\n\\] Where:\n\n\\(\\textbf{X}\\) is the data matrix of size \\(n \\times p\\)\n\\(\\textbf{U}\\) is the left singular vectors of size \\(n \\times n\\)\n\\(\\textbf{V}^\\top\\) is the right singular vectors of size \\(p \\times p\\)\n\\(\\Sigma\\) is the singular values matrix of size \\(n \\times p\\)\n\nThe two orthogonal matrices, \\(\\textbf{U}\\) and \\(\\textbf{V}^\\top\\), contain information about the column or row space of \\(\\textbf{X}\\). \\(\\textbf{U}\\) contains information about the column space of \\(\\textbf{X}\\) and summarizes information of different users based on movies. \\(\\textbf{V}^\\top\\) contains information about the row space of \\(\\textbf{X}\\) and summarizes information of different movies based on users. The columns of \\(\\textbf{U}\\) and rows of \\(\\textbf{V}^\\top\\) are hierarchically arranged, such that the first column of \\(\\textbf{U}\\) and the first row of \\(\\textbf{V}^\\top\\) describe more variance than the second column of \\(\\textbf{U}\\) and the second row of \\(\\textbf{V}^\\top\\) and so on.\nThe importance of the various columns of \\(\\textbf{U}\\) and rows of \\(\\textbf{V}^\\top\\), therefore, is encoded in the diagonal matrix, \\(\\Sigma\\), in which the values are arranged in descending order by importance, such that, the first value \\(\\sigma_1\\) is used to explain more variance in \\(\\textbf{X}\\) than the second value \\(\\sigma_2\\), etc. For these matrices, the first \\(p\\) columns can be thought of as as factors or PCs that describe the interactions between users and movies.\nIn cases where there are more users (\\(n\\)) than predictors (\\(p\\)), calculating the SVD of \\(\\textbf{X}\\) means that only the first \\(p\\) columns of \\(\\textbf{U}\\) and the first \\(p \\times p\\) block of \\(\\Sigma\\) are selected (the \\(p \\times p\\) matrix \\(\\textbf{V}^\\top\\) remains unchanged). Given the hierarchical structure of \\(\\Sigma\\), there will be very small singular values toward the bottom that are not very important. We can, therefore, truncate the calculation to only include the first \\(k\\) singular values of \\(\\Sigma\\) . These \\(k\\) values are the factors that describe most of the variation in \\(\\textbf{X}\\). When we truncate to \\(k\\) factors, we approximate the matrix \\(\\textbf{X}\\) through the following formula:\n\\[\n\\textbf{X}_{n \\times p} \\approx \\textbf{U}_{n \\times k} \\times \\Sigma_{k \\times k} \\times  \\textbf{V}^\\top_{k \\times n} = \\hat{\\textbf{X}}_{n \\times p}\n\\]\nTherefore, using SVD for dimension reduction is more efficient than performing a PCA. To properly calculate the predicted values using SVD, the data must be centered. When the data in matrix \\(\\textbf{X}\\) is centered, then \\(\\textbf{X}^\\top\\textbf{X}/(n - 1)\\) is equal to the covariance matrix (for further explanation of SVD, see Brunton & Kutz, 2019; Siregar, n.d.; Visual Kernel, 2022).\n\n\nProblem with Sparse Data\nAs mentioned above, SVD is calculated directly from the data matrix. This means that we need to transform our current long-format data into a wide-format matrix. This is different from how we were able to calculate movie and user effects earlier by grouping a tidy data frame by movieId or userId and then calculating the least squares estimates. To illustrate, see the sample table below of some movies shown earlier.\n\nlong_sample &lt;- test_set |&gt; \n  filter(movieId %in% c(2959, 260, 1210, 25, 1704, 2959, 924, 1208)) |&gt; \n  select(userId, movieId, rating) |&gt; \n  slice_sample(n = 10) |&gt; \n  remove_rownames()\n\n\nlong_sample |&gt; \n  as.matrix()\n##       userId movieId rating\n##  [1,]   1690    1210      4\n##  [2,]  61409    1704      4\n##  [3,]  36823    1210      4\n##  [4,]  27385    1210      4\n##  [5,]  41012     260      5\n##  [6,]  46423    1210      5\n##  [7,]  39063    1210      4\n##  [8,]   7372    1704      1\n##  [9,]  11374     260      4\n## [10,]  63145      25      5\n\nTo perform SVD on this data, we would need to transform the original data into a user-item matrix. However, even for highly popular films, we see that there are many missing values.\n\nuser_item_sample_matrix &lt;- long_sample |&gt; \n  pivot_wider(id_cols = userId,\n              names_from = movieId,\n              values_from = rating) |&gt; \n  column_to_rownames(\"userId\") |&gt; \n  as.matrix()\n\n\nuser_item_sample_matrix\n##       1210 1704 260 25\n## 1690     4   NA  NA NA\n## 61409   NA    4  NA NA\n## 36823    4   NA  NA NA\n## 27385    4   NA  NA NA\n## 41012   NA   NA   5 NA\n## 46423    5   NA  NA NA\n## 39063    4   NA  NA NA\n## 7372    NA    1  NA NA\n## 11374   NA   NA   4 NA\n## 63145   NA   NA  NA  5\n\nEven though SVD requires less processing power than PCA, SVD does not work well on sparse matrices. If we were to calculate the entire matrix of \\(n\\) users by \\(p\\) movies, then we would find that less than 2% of data actually contains an observed rating.\n\ntotal_cells &lt;- n_distinct(edx$userId) * n_distinct(edx$movieId)\nvalid_cells &lt;- nrow(edx)\nsparsity &lt;- valid_cells/total_cells\n\n\ntotal_cells\n## [1] 746087406\nvalid_cells\n## [1] 9000055\nsparsity # proportion of non-missing values\n## [1] 0.012063\n\nIn addition to having sparse data, we want to calculate the truncated SVD using \\(k\\) singular values, or factors. R’s svd() function is not optimal, because (a) it cannot include missing values and (b) it would attempt to return the complete set of singular values. Even if we imputed the missing values, we would likely run out of computing power and R would crash. A better option for our data is the irlba() function, which returns the number of singular vectors \\(k\\) specified, even if \\(k\\) is less than the maximum value of \\(p\\) predictors.\nBefore we start, we need to make sure that userId and movieId are stored in the data as factors. We, then, make sure that both test and training sets include all the unique IDs from both columns. Although we did a similar step earlier, we did not need to factorize the ID columns in this manner until now.\n\nuser_levels  &lt;- sort(unique(train_set$userId))\nmovie_levels &lt;- sort(unique(train_set$movieId))\n\ntrain_set &lt;- train_set |&gt; \n  mutate(userId_x = as.integer(factor(userId)),\n         movieId_x = as.integer(factor(movieId))) |&gt; \n  mutate(userId_x = match(userId, user_levels),\n         movieId_x = match(movieId, movie_levels))\n\ntest_set &lt;- test_set |&gt; \n  mutate(userId_x = as.integer(factor(userId)),\n         movieId_x = as.integer(factor(movieId))) |&gt; \n  mutate(userId_x = match(userId, user_levels),\n         movieId_x = match(movieId, movie_levels))\n\nsaveRDS(train_set, \"../9_Capstone/data/train-set.rds\")\nsaveRDS(test_set,  \"../9_Capstone/data/test-set.rds\")"
  },
  {
    "objectID": "movielens.html#movie-effect-with-svd",
    "href": "movielens.html#movie-effect-with-svd",
    "title": "Capstone MovieLens Project",
    "section": "Movie Effect with SVD",
    "text": "Movie Effect with SVD\nTo start with our SVD models, we will remove the movie-bias effect from the data prior to calculating the SVD. To save memory space and increase processing power, we will switch from the dplyr and tidyr packages’ syntax to the data.table package syntax. After converting both datasets from data.frame objects to data.table objects, we then center the ratings around the average movie mean in the training set. Next, we use the Matrix package to create a sparse matrix object.\n\nsetDT(train_set)\nsetDT(test_set)\n\ncol_means &lt;- train_set[, .(mean_ratings = mean(rating)), by = movieId]\ntrain_set[col_means, centered := rating - mean_ratings, on = \"movieId\"]\n\ny &lt;- sparseMatrix(\n  i = as.integer(factor(train_set$userId)),\n  j = as.integer(factor(train_set$movieId)),\n  x = train_set$centered,\n  dimnames = list(\n    user = levels(factor(train_set$userId)), \n    movie = levels(factor(train_set$movieId)))\n)\n\nThe irlba() function from the irlba package finds a few approximate singular values and corresponding singular vectors of the matrix y. Since we do not want to calculate the entire number of \\(p\\) movies, we set the number of right singular vectors and left singular vectors that we want to estimate. In this case, we select \\(k = 50\\) as our arbitrary starting point.\nTo calculate the predicted matrix, pred_mat, we use the SVD formula to multiply the matrices together. To extract the predicted centered ratings for the test dataset, pred_centered extracts the corresponding cell from pred_mat using the user’s row position and movie’s column position. To extract the correct column means for the test dataset’s movies, movie_means uses match() to find each test movie’s position in the col_means table, retrieving the corresponding mean rating. The final predicted values, pred, are calculated by adding the centered predicted values to their corresponding movie mean. Finally, to benchmark the time it takes to run the algorithm, we track processing time.\n\nt1 &lt;- Sys.time()\n\ns &lt;- irlba(y, nv = 50, nu = 50)\n\npred_mat &lt;- s$u %*% diag(s$d) %*% t(s$v)\n\npred_centered &lt;- pred_mat[cbind(test_set$userId_x, test_set$movieId_x)]\n\nmovie_means &lt;- col_means$mean_ratings[match(test_set$movieId,col_means$movieId)]\n\npred &lt;- pred_centered + movie_means\n\nSys.time() - t1\n\n\n## Time difference of 47.02883 secs\n\nAfter running our SVD model, we see that the accuracy has taken a step backwards. This SVD movie-effect RMSE is better than its least-squares counterpart (fit1_rmse = 0.9441), but it is not better than the models where we also removed user bias. So far the regularized movie and user effects model had the best RMSE, but the difference between the regularized (0.8654) and non-regularized (0.8659) models are negligible. Next we will attempt to use SVD after removing both movie and user biases.\n\nfit4_rmse &lt;- sqrt(mean((pred - test_set$rating)^2))\n\n\nfit4_rmse\n## [1] 0.8874975"
  },
  {
    "objectID": "movielens.html#movie-and-user-effects-with-svd",
    "href": "movielens.html#movie-and-user-effects-with-svd",
    "title": "Capstone MovieLens Project",
    "section": "Movie and User Effects with SVD",
    "text": "Movie and User Effects with SVD\nIn removing both the movie and user bias, our goal is to use SVD to identify the most important factors beyond systematic movie and user effects. The resulting patterns calculated from the SVD (i.e., the predicted residuals) can, then, be used to add to the final predicted values, including the global average, \\(\\hat{\\mu}\\), movie effects, \\(b_i\\), and user effects, \\(b_u\\). First, we calculate mu_hat from the training set. Next, we calculate the movie_means as we had done before. To calculate user means, we first join movie_means with the training set and, then, remove the global average and movie means from each user’s ratings.\n\nmu_hat &lt;- mean(train_set$rating)\n\nmovie_means &lt;- train_set[, .(b_i = mean(rating - mu_hat)), by = movieId]\n\nuser_means &lt;- train_set[movie_means, on = \"movieId\"][\n  , .(b_u = mean(rating - mu_hat - b_i)), by = userId]\n\nTo remove the user and movie bias from the ratings, we copy the training dataset to train_resid to indicate that it will contain residuals. We, then, join the \\(b_i\\) and \\(b_u\\) terms based on movieId and userId, respectively. Finally, we calculate the residuals by subtracting the global mean, movie means, and user means from each user’s ratings. Using these columns, we create a sparse Matrix object that includes the residuals ready for SVD calculations.\n\ntrain_resid &lt;- train_set\ntrain_resid[movie_means, b_i := i.b_i, on = \"movieId\"]\ntrain_resid[user_means, b_u := i.b_u, on = \"userId\"]\ntrain_resid[, residual := rating - mu_hat - b_i - b_u]\n\ny &lt;- sparseMatrix(\n  i = train_resid$userId_x,\n  j = train_resid$movieId_x,\n  x = train_resid$residual,\n  dimnames = list(\n    user = levels(factor(train_resid$userId)),\n    movie = levels(factor(train_resid$movieId))\n  ))\n\nTo calculate the approximate singular values and corresponding singular vectors, we use the irlba() function, and as before, we select \\(k = 50\\) as the number of right and left singular vectors to estimate. Once the singular values are estimated, we calculate the predicted matrix using the SVD formula. Using the correct row and column position for each user and movie, we store the extracted predicted matrix values into pred_residuals. Similarly, we create vectors for movie and user effects based upon their movieId and userId positioning in the test dataset. Finally, we use the clamp() function created earlier to estimate the predicted values, by adding together the global mean, movie effects, user effects, and predicted residuals.\n\nt1 &lt;- Sys.time()\n\ns &lt;- irlba(y, nv = 50, nu = 50)\npred_mat &lt;- s$u %*% diag(s$d) %*% t(s$v)\n\npred_residuals &lt;- pred_mat[cbind(test_set$userId_x, test_set$movieId_x)]\n\ntest_b_i &lt;- movie_means$b_i[match(test_set$movieId, movie_means$movieId)]\n\ntest_b_u &lt;- user_means$b_u[match(test_set$userId, user_means$userId)]\n\npred &lt;- clamp(mu_hat + test_b_i + test_b_u + pred_residuals)\n\nSys.time() - t1\n\n\n## Time difference of 1.539259 mins\n\nThe RMSE has greatly improved after removing both movie and user effects before calculating the SVD. Next, we will determine whether the accuracy could be improved with regularization.\n\nfit5_rmse &lt;- RMSE(pred, test_set$rating)\n\n\nfit5_rmse\n## [1] 0.8388077"
  },
  {
    "objectID": "movielens.html#regularized-movie-user-effects-with-svd",
    "href": "movielens.html#regularized-movie-user-effects-with-svd",
    "title": "Capstone MovieLens Project",
    "section": "Regularized Movie & User Effects with SVD",
    "text": "Regularized Movie & User Effects with SVD\nIn the previous model, we removed both the movie and user effects from the data matrix before calculating the singular values. However, could we optimize the number of \\(k\\) singular vectors to estimate? We could also add a penalty term \\(\\lambda\\) to the sample size when calculating the movie and user effects. In this model, we attempt to find an optimal \\(k\\) and \\(\\lambda\\) value that will yield the lowest RMSE. To do this, we create a function that calculates movie and user effects at different levels of \\(\\lambda\\) and then use those calculations to estimate \\(k\\) singular values and corresponding singular vectors.\nTo start, we select values of lambda from 0 to 5 in increments of 0.5 and values of k at either 50 or 100. Below we use the expand.grid() function to create a data frame from all combinations of lambdas and k_values. The final function will perform an SVD for each combination of lambda and k for a total of 22 different models.\n\nlambdas &lt;- seq(0, 5, 0.50)\nk_values &lt;- c(50, 100)\n\ngrid &lt;- expand.grid(lambda = lambdas, k = k_values)\n\ngrid\n##    lambda   k\n## 1     0.0  50\n## 2     0.5  50\n## 3     1.0  50\n## 4     1.5  50\n## 5     2.0  50\n## 6     2.5  50\n## 7     3.0  50\n## 8     3.5  50\n## 9     4.0  50\n## 10    4.5  50\n## 11    5.0  50\n## 12    0.0 100\n## 13    0.5 100\n## 14    1.0 100\n## 15    1.5 100\n## 16    2.0 100\n## 17    2.5 100\n## 18    3.0 100\n## 19    3.5 100\n## 20    4.0 100\n## 21    4.5 100\n## 22    5.0 100\n\nPerforming 22 SVD calculations on this large of a dataset will require a lot of memory and computing power. Therefore, we will use the doParallel package to help. We will first create two copies of R running in parallel, called “clusters.” To ensure the two copies of R can communicate to each other we use the registerDoParallel() function. To ensure the essential objects from the R environment are copied to both copies of R, we use the clusterExport() function to copy the training and test datasets, as well as the average rating of training set.\n\nmu_hat &lt;- mean(train_set$rating)\n\nnc &lt;- 2\ncl &lt;- makeCluster(nc)\nregisterDoParallel(cl)\n\nclusterEvalQ(cl, {\n  library(data.table)\n  library(Matrix)\n  library(irlba)\n})\n\nclusterExport(cl, c(\"train_set\", \"test_set\", \"mu_hat\"))\n\nIn order to calculate the RMSE values of the 22 different models, we write a function, rmses() that takes in lambda and k values as arguments and returns the RMSE for each model. The function does the following:\n\nmovie_means_reg: Calculate the \\(b_i\\) movie effects with an added penalty term, lambda, to the sample size in the denominator when calculating the mean.\nuser_means_reg: Calculate the \\(b_u\\) user effects using the added penalty term as above.\ntrain_resid: Copy the training set as before to indicate that this will be the dataset that contains residuals.\n\nJoin the regularized movie and user means to the train_resid dataset so that we can, then, calculate the residual for each observation.\nCreate the residual column in train_resid by subtracting the global mean, movie bias, and user bias from each observed rating.\n\ny: Using the sparseMatrix() function, create a sparse matrix necessary for the irbla() function.\ns: Calculate the approximate singular values and singular vectors using the irbla() function. We use the function’s argument variable k for truncating the SVD.\npred_residuals: Calculate the predicted residual matrix using the SVD equation.\ntest_residuals: Using the users’ and movies’ row/column positioning in the test dataset, extract the predicted residuals.\ntest_b_i: Create a vector for the movie effects based upon each movie ID’s position in the test dataset\ntest_b_u: Create a vector for the user effects based on user ID’s test set position.\npred: Use the clamp() function to save the predicted ratings according to the final model’s formula, by adding together the global average, movie effect, user effect, and predicted residuals.\nFinally, return the RMSE by taking the square root of the mean of the squared residuals between the predicted and actual movie ratings.\n\n\nrmses &lt;- function(lambda, k){\n  movie_means_reg &lt;- train_set[, .(b_i = sum(rating - mu_hat)/(.N + lambda)), by = movieId_x]\n  \n  user_means_reg &lt;- train_set[movie_means_reg, on = \"movieId_x\"][\n    , .(b_u = sum(rating - mu_hat - b_i)/(.N + lambda)), by = userId_x]\n  \n  train_resid &lt;- train_set\n  train_resid[movie_means_reg, b_i := i.b_i, on = \"movieId_x\"]\n  train_resid[user_means_reg, b_u := i.b_u, on = \"userId_x\"]\n  train_resid[, residual:= rating - mu_hat - b_i - b_u]\n  \n  y &lt;- sparseMatrix(\n    i = train_resid$userId_x,\n    j = train_resid$movieId_x,\n    x = train_resid$residual\n  )\n  \n  s &lt;- irlba(y, nv = k, nu = k)\n  pred_residuals &lt;- s$u %*% diag(s$d) %*% t(s$v)\n  \n  test_residuals &lt;- pred_residuals[cbind(test_set$userId_x, test_set$movieId_x)]\n  \n  test_b_i &lt;- movie_means_reg[.(test_set$movieId_x), b_i, on = \"movieId_x\"]\n  test_b_u &lt;- user_means_reg[.(test_set$userId_x), b_u, on = \"userId_x\"]\n  \n  pred &lt;- clamp(mu_hat + test_b_i + test_b_u + test_residuals)\n\n  RMSE(pred, test_set$rating)\n}\n\nNow that we have set up our parallel processing and written our function, we use the foreach package (attached from the doParallel package) to evaluate in parallel the rmses() function we created above for each row of our grid data frame. Within our rmses() function, we pass through the \\(i\\)th row of the lambda and k columns. We use the .combine = c argument to create a vector of all the RMSE values calculated from each of the 22 models. As usual, we document the processing time using Sys.time(). Note that this code chunk will take between 30-40 minutes of processing time. If possible, close all programs running in the background before running.\n\nt1 &lt;- Sys.time()\n\nresults &lt;- foreach(i = 1:nrow(grid), .combine = c) %dopar% {\n  rmses(grid$lambda[i], grid$k[i])\n}\n\nSys.time() - t1\n\n\n## Time difference of 34.44416 mins\n\nWe store the results of all 22 RMSE calculations into a new column in the grid data frame. When we are finished with the entire process we close the created cluster using the stopCluster() and stopImplicitCluster() functions. Lastly, we plot the results.\n\ngrid$rmse &lt;- results \n\nstopCluster(cl)\nstopImplicitCluster()\n\nlambda_plot &lt;- ggplot(grid |&gt; mutate(k = factor(k))) +\n  geom_point(aes(lambda, rmse, color = k))\n\nlambda_plot\n\n\nAs shown in the plot, the lowest RMSE value uses parameters of \\(\\lambda = 5\\) and \\(k = 50\\). Despite the computationally intensive process, this final model is not much better than the non-regularized SVD movie and user effect model we calculated earlier. However, we will select this model with the given regularization parameters to predict movie ratings of the final hold-out test set.\n\nlambda_m6 &lt;- grid[which.min(grid$rmse),\"lambda\"]\nk_m6 &lt;- grid[which.min(grid$rmse),\"k\"]\nfit6_rmse &lt;- min(grid$rmse)\n\n\nlambda_m6\n## [1] 5\nk_m6\n## [1] 50\nfit6_rmse\n## [1] 0.8382405"
  }
]