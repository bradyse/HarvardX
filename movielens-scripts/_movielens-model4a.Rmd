## Dimension Reduction and SVD

### Problem with Unexplained Variance

Despite estimating movie-to-movie differences through $b_i$ and user-to-user differences through $b_u$, we are not estimating variation related to groups of movies and users with similar rating patterns. If we look at the residuals for some popular films, we see that non-zero correlations exist. This is expected for movies of the same franchise (Plot A) or the same genre (Plot B). We also see this for overlapping genres (Plot C), as well as non-overlapping genres (Plot D). Because residuals are supposed to be uncorrelated with each other, this suggests that our current model is not capturing certain commonalities between movies and users. Moreover, the commonalities that exist are not readily apparent from knowing the genre, release date, or title.

```{r include=FALSE, eval=FALSE}
fit3_b_i_hat <- train_set |> 
  group_by(movieId) |> 
  summarize(b_i_hat = sum(rating - mu_hat)/(n()+lambda_m3))
     
fit3_b_u_hat <- train_set |> 
  left_join(fit3_b_i_hat, by="movieId") |> 
  group_by(userId) |> 
  summarize(b_u_hat = sum(rating - b_i_hat - mu_hat)/(n()+lambda_m3))

fit3_resid <- test_set |> 
  left_join(movie_avgs, by="movieId") |> 
  left_join(user_avgs, by="userId") |> 
  mutate(predicted_ratings = clamp(mu_hat + b_i_hat + b_u_hat),
         residuals = rating - predicted_ratings)
```

```{r include=FALSE, eval=FALSE}
frequently_rated_films <- fit3_resid |> 
  group_by(movieId) |> 
  reframe(n = n(), title, b_i_hat, genres) |> 
  distinct() |> 
  filter(b_i_hat > 0 & n > 1000)

popular_movieIds <- frequently_rated_films |> pull(movieId)

popular_wide <- fit3_resid |> 
  filter(movieId %in% popular_movieIds) |> 
  pivot_wider(id_cols = c(userId),
              names_from = movieId,
              values_from = residuals,
              names_prefix = "m")

popular_rs <- cor(popular_wide[,-1], 
                  method = "spearman",
                  use = "pairwise.complete.obs") |> 
  as.data.frame() |> 
  rownames_to_column() |> 
  pivot_longer(-rowname) |> 
  filter(between(value, .6, .99)) |> 
  arrange(-value) |> 
  rename(x = rowname, y = name) |> 
  mutate(across(c(1:2), ~ as.integer(str_remove(., "m"))))

popular_movies_rs <- left_join(popular_rs, 
                select(frequently_rated_films, movieId, title),
                by = c("x" = "movieId")) |> 
  rename(x_title = title) |> 
  left_join(select(frequently_rated_films, movieId, title),
                by = c("y" = "movieId")) |> 
  rename(y_title = title)
```

```{r include=FALSE, eval=FALSE}
p1 <- fit3_resid |> 
  filter(movieId %in% c("260", "1210")) |> 
  pivot_wider(id_cols = c(userId),
              names_from = movieId,
              values_from = residuals,
              names_prefix = "m") |> 
  na.omit() |> 
  ggplot(aes(x = m260, y = m1210)) +
  geom_point() +
  geom_smooth() +
  labs(x = "A New Hope (a.k.a. Star Wars) (1977)",
       y = "Return of the Jedi (1983)",
       title = "Same Movie Franchise",
       subtitle = "Star Wars")

p2 <- fit3_resid |> 
  filter(movieId %in% c("25", "1704")) |> 
  pivot_wider(id_cols = c(userId),
              names_from = movieId,
              values_from = residuals,
              names_prefix = "m") |> 
  na.omit() |> 
  ggplot(aes(x = m1704, y = m25)) +
  geom_point() +
  geom_smooth() +
  labs(x = "Good Will Hunting (1997)",
       y = "Leaving Las Vegas (1995)",
       title = "Same Genre",
       subtitle = "Drama|Romance")

p3 <- fit3_resid |> 
  filter(movieId %in% c("25", "2959")) |> 
  pivot_wider(id_cols = c(userId),
              names_from = movieId,
              values_from = residuals,
              names_prefix = "m") |> 
  na.omit() |> 
  ggplot(aes(x = m2959, y = m25)) +
  geom_point() +
  geom_smooth()+
  labs(x = "Fight Club (1999)",
       y = "Leaving Las Vegas (1995)",
       title = "Overlapping Genres",
       subtitle = "Action|Crime|Drama|Thriller vs. Drama|Romance")

p4 <- fit3_resid |> 
  filter(movieId %in% c("924", "1208")) |> 
  pivot_wider(id_cols = c(userId),
              names_from = movieId,
              values_from = residuals,
              names_prefix = "m") |> 
  na.omit() |> 
  ggplot(aes(x = m924, y = m1208)) +
  geom_point() +
  geom_smooth() +
  labs(x = "2001: A Space Odyssey (1968)",
       y = "Apocalypse Now (1979)",
       title = "Non-overlapping Genres",
       subtitle = "Adventure|Sci-Fi vs. Action|Drama|War")

```

```{r echo=FALSE, fig.dim=c(10,6), eval=FALSE}
p1 + p2 + p3 + p4 +
  patchwork::plot_annotation(title = "Residuals of Popular Movies",
                             tag_levels = "A")

```

![](images/residuals-popular-movies.png){width="75%"}

### PCA via SVD as Method for Dimension Reduction

Because we have 10,677 predictors (movies) in our model, a popular method for reducing the number of predictors is through *principal component analysis* (PCA). PCA seeks to identify groups of predictors, whereby predictors within groups are relatively similar and the groups are relatively independent of one another. In this case, the groups of predictors are called principal components (PCs).

Traditionally, PCs are extracted using eigenvalue decomposition in which the $p \times p$ covariance matrix (i.e., the matrix of correlations between predictors) is computed and, then, transformed into a matrix of $p \times p$ eigenvalues (i.e., the variance explained by each PC) using matrix factorization via matrix algebra. See [@penn](https://online.stat.psu.edu/stat505/lesson/4/4.5) for an explanation of eigenvalue calculations.

Because calculating the covariance matrix is computationally intensive on large datasets, an alternative approach to extracting PCs is *singular value decomposition* (SVD). With SVD, we can avoid calculating the covariance matrix altogether and instead transform the dataset matrix, $\textbf{X}$, of size $n \times p$ (number of $n$ users by number of $p$ movies) into three special matrices. Two matrices are *orthogonal* (rotated): $\textbf{U}$ and $\textbf{V}^\top$. A third matrix, $\Sigma$ is *diagonal* matrix.

The mathematical formula for SVD then is:

$$
\textbf{X} = \textbf{U}\Sigma\textbf{V}^\top
$$ Where:

-   $\textbf{X}$ is the data matrix of size $n \times p$
-   $\textbf{U}$ is the left singular vectors of size $n \times n$
-   $\textbf{V}^\top$ is the right singular vectors of size $p \times p$
-   $\Sigma$ is the singular values matrix of size $n \times p$

The two orthogonal matrices, $\textbf{U}$ and $\textbf{V}^\top$, contain information about the column or row space of $\textbf{X}$. $\textbf{U}$ contains information about the column space of $\textbf{X}$ and summarizes information of different users based on movies. $\textbf{V}^\top$ contains information about the row space of $\textbf{X}$ and summarizes information of different movies based on users. The columns of $\textbf{U}$ and rows of $\textbf{V}^\top$ are hierarchically arranged, such that the first column of $\textbf{U}$ and the first row of $\textbf{V}^\top$ describe more variance than the second column of $\textbf{U}$ and the second row of $\textbf{V}^\top$ and so on.

The importance of the various columns of $\textbf{U}$ and rows of $\textbf{V}^\top$, therefore, is encoded in the diagonal matrix, $\Sigma$, in which the values are arranged in descending order by importance, such that, the first value $\sigma_1$ is used to explain more variance in $\textbf{X}$ than the second value $\sigma_2$, etc. For these matrices, the first $p$ columns can be thought of as as factors or PCs that describe the interactions between users and movies.

In cases where there are more users ($n$) than predictors ($p$), calculating the SVD of $\textbf{X}$ means that only the first $p$ columns of $\textbf{U}$ and the first $p \times p$ block of $\Sigma$ are selected (the $p \times p$ matrix $\textbf{V}^\top$ remains unchanged). Given the hierarchical structure of $\Sigma$, there will be very small singular values toward the bottom that are not very important. We can, therefore, truncate the calculation to only include the first $k$ singular values of $\Sigma$ . These $k$ values are the factors that describe most of the variation in $\textbf{X}$. When we truncate to $k$ factors, we approximate the matrix $\textbf{X}$ through the following formula:

$$
\textbf{X}_{n \times p} \approx \textbf{U}_{n \times k} \times \Sigma_{k \times k} \times  \textbf{V}^\top_{k \times n} = \hat{\textbf{X}}_{n \times p}
$$

Therefore, using SVD for dimension reduction is more efficient than performing a PCA. To properly calculate the predicted values using SVD, the data must be centered. When the data in matrix $\textbf{X}$ is centered, then $\textbf{X}^\top\textbf{X}/(n - 1)$ is equal to the covariance matrix [for further explanation of SVD, see @brunton; @siregar; @vkernel].

### Problem with Sparse Data

As mentioned above, SVD is calculated directly from the data matrix. This means that we need to transform our current long-format data into a wide-format matrix. This is different from how we were able to calculate movie and user effects earlier by grouping a tidy data frame by `movieId` or `userId` and then calculating the least squares estimates. To illustrate, see the sample table below of some movies shown earlier.

```{r eval=FALSE, include=TRUE}
long_sample <- test_set |> 
  filter(movieId %in% c(2959, 260, 1210, 25, 1704, 2959, 924, 1208)) |> 
  select(userId, movieId, rating) |> 
  slice_sample(n = 10) |> 
  remove_rownames()
```

```{r}
long_sample |> 
  as.matrix()
```

To perform SVD on this data, we would need to transform the original data into a user-item matrix. However, even for highly popular films, we see that there are many missing values.

```{r eval=FALSE, include=TRUE}
user_item_sample_matrix <- long_sample |> 
  pivot_wider(id_cols = userId,
              names_from = movieId,
              values_from = rating) |> 
  column_to_rownames("userId") |> 
  as.matrix()
```

```{r}
user_item_sample_matrix
```

Even though SVD requires less processing power than PCA, SVD does not work well on sparse matrices. If we were to calculate the entire matrix of $n$ users by $p$ movies, then we would find that less than 2% of data actually contains an observed rating.

```{r eval=FALSE, include=TRUE}
total_cells <- n_distinct(edx$userId) * n_distinct(edx$movieId)
valid_cells <- nrow(edx)
sparsity <- valid_cells/total_cells

```

```{r}
total_cells
valid_cells
sparsity # proportion of non-missing values

```


In addition to having sparse data, we want to calculate the truncated SVD using $k$ singular values, or factors. R's `svd()` function is not optimal, because (a) it cannot include missing values and (b) it would attempt to return the complete set of singular values. Even if we imputed the missing values, we would likely run out of computing power and R would crash. A better option for our data is the `irlba()` function, which returns the number of singular vectors $k$ specified, even if $k$ is less than the maximum value of $p$ predictors.

Before we start, we need to make sure that `userId` and `movieId` are stored in the data as factors. We, then, make sure that both test and training sets include all the unique IDs from both columns. Although we did a similar step earlier, we did not need to factorize the ID columns in this manner until now.

```{r eval=FALSE, include=TRUE}
user_levels  <- sort(unique(train_set$userId))
movie_levels <- sort(unique(train_set$movieId))

train_set <- train_set |> 
  mutate(userId_x = as.integer(factor(userId)),
         movieId_x = as.integer(factor(movieId))) |> 
  mutate(userId_x = match(userId, user_levels),
         movieId_x = match(movieId, movie_levels))

test_set <- test_set |> 
  mutate(userId_x = as.integer(factor(userId)),
         movieId_x = as.integer(factor(movieId))) |> 
  mutate(userId_x = match(userId, user_levels),
         movieId_x = match(movieId, movie_levels))

saveRDS(train_set, "../9_Capstone/data/train-set.rds")
saveRDS(test_set,  "../9_Capstone/data/test-set.rds")
```