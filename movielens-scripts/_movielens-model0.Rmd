## Creating Test and Training Sets for Model Testing

Even though we created the training (`edx`) and testing (`final_holdout_test`) datasets, we will only use the `final_holdout_test` dataset after choosing the final machine learning algorithm. Therefore, we will use the `edx` dataset to create training and testing partitions to be used during model training. Using the `caret` package, we generate the indexes for randomly dividing the data into two  parts. Then, we create the training and test sets using the indexes generated. The methodology here uses the same logic that the HarvardX Team used above for creating the `edx` and `final_holdout_test` sets.

First, we use the caret's `createDataPartition()` function to generate indices that are used to subset 90% of the `edx` dataset into the main training set. Then, we create a `temp` dataset that will serve as the basis of the indices generated by the data partition, representing 10% of `edx` data. To ensure that all users and movies appear in the test set, we use dplyr's `semi_join()` function to return all rows from `temp` with a match in `train_set`. Next, we use `anti_join()` to return all rows from `temp` that don't appear in `test_set`. Finally, we use `rbind()`to combine the removed rows with the matched rows to produce our final `test_set`.

```{r eval=FALSE, include=TRUE}
y = edx$rating
set.seed(7)
test_index <- caret::createDataPartition(y, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]

test_set <- temp |> 
  semi_join(train_set, by = "movieId") |> 
  semi_join(train_set, by = "userId")

removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)

saveRDS(train_set, "../../9_Capstone/data/train-set.rds")
saveRDS(test_set,  "../../9_Capstone/data/test-set.rds")
```

## RMSE

To test model accuracy, we will use a commonly used metric for evaluating the extent to which predicted ratings matched actual ratings. When Netflix opened a competition for the best algorithm to predict user ratings [@nyt], they used the root mean square error (RMSE) to compare the best algorithm against their current algorithm used at the time [@kaggle]. The winning algorithm achieved an RMSE of of 0.8712 [@bellkor07].

RMSE is often used as an evaluation metric of regression model accuracy. It answers the question: How accurate are a model's predicted values? Because of how it is calculated, RMSE directly measures prediction error in the same units as the outcome variable. The \$1 million prize-winning algorithm, therefore, estimated actual movie ratings within 0.8712 stars on the 5-star rating scale.

The mathematical formula for calculating RMSE is:

$$
\mathrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$

here:

-   $y_i$ is the actual rating for observation $i$,
-   $\hat{y}_i$ is the predicted rating for observation $i$,
-   $n$ is the total number of observations

In effect, RMSE calculates the residuals, squares them, finds the mean of the squared residuals, and takes the square root.

Then, a simplified formula of RMSE is:

$$
\mathrm{RMSE} = \sqrt{\text{mean}\Big((y_i - \hat{y}_i)^2\Big)}
$$

here:

-   $y_i$ is the vector of actual ratings
-   $\hat{y}_i$ is the vector of predicted values

We can write a function in R to calculate RMSE. The predicted ratings will be generated from whatever model we choose based upon the training datset, and the actual ratings will come from the testing dataset.

```{r}
RMSE <- function(actual_ratings, predicted_ratings){
     sqrt(mean((actual_ratings - predicted_ratings)^2))
}
```

## Average Model

To benchmark our models, we estimate the naive RMSE using the average rating in the training dataset to predict the test set's ratings. This code was adapted from [*Introduction to Data Science*](https://rafalab.dfci.harvard.edu/dsbook/large-datasets.html#recommendation-systems-as-a-machine-learning-challenge) [@irizarry].

Our average model would look like this: $Y_{u,i} = \mu + \varepsilon_{u,i}$ with $\varepsilon_{u,i}$ independent errors sampled from the same distribution centered at 0 and $\mu$, the actual "true" rating for all movies.

Since we don't know the "true" rating for all movies, we predict all unknown ratings with $\hat{\mu}$ using the average of all ratings from the training set. Given this model, our benchmark RMSE is 1.0606.

```{r eval=FALSE, include=TRUE}
mu_hat <- mean(train_set$rating)
naive_rmse <- RMSE(test_set$rating, mu_hat)
```

```{r}
naive_rmse
```


By plotting the residuals, we see a plot that looks identical to the histogram of the ratings. This is because we have effectively centered all ratings around the mean.

```{r eval=FALSE, include=TRUE}
naive_df <- select(test_set, rating) |> 
  mutate(residuals = rating - mu_hat)

select(naive_df, residuals) |> 
  ggplot(aes(x = residuals)) +
  geom_histogram(binwidth = 0.5,
                 color = "lightgray") +
  geom_vline(xintercept = 0,
             linetype = 2)

```

![](images/naive-residuals.png){width="75%"}
